\documentclass[11pt,letterpaper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{upgreek}
\usepackage{apacite}
\usepackage{ntheorem}
\usepackage{enumitem}
\usepackage{gensymb}
\usepackage[ruled,vlined, linesnumbered]{algorithm2e}
\graphicspath{ {./Figures/} }
\newtheorem{hyp}{Hypothesis}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{
	Kho, Lee\\
	\textit{ltk224}
	\and
	Peeters, Jonas\\
	\textit{jp5642}
}
\title{DS-GA 1013: Project\\Intelligible Principal Component Analysis}

\usepackage[strings]{underscore}
\usepackage{fancyhdr}
\fancypagestyle{plain}{
    \fancyhf{}
    \lhead{Kho \& Peeters}
    \chead{\thepage}
    \rhead{DS-GS 1013}
}
\pagestyle{plain}

\setlength\parindent{0pt}
\allowdisplaybreaks

\input{macros}

\begin{document}
\maketitle
\section*{Introduction}

Principal component analysis (PCA) is a popular dimension-reduction technique that has wide applications across many fields. The goal of PCA is to reduce the dimension of the data while preserving as much variance as possible. PCA does this by computing orthogonal linear combinations of the original variables, called the principal components (PCs), such that the variance along these components are maximized.  The resulting PCs are ordered from highest variance to lowest variance, and thus the first few principal components will retain the most variance in the data.\\
\\
One major limitation of PCA is that the resulting linear combinations of the original $p$ variables are difficult to interpret. This is due to the fact that, in most cases, the loadings of the resulting PCs are nonzero. This is particularly challenging when the number of variables $p$ is much larger than the number of examples $n$ (commonly called “High Dimension, Low Sample Size” data, or HDLSS), which is the case for many modern data sets. Consequently, much effort has gone into developing methods of approximating PCs with fewer than the $p$ original variables.\\
\\
In this paper, we will explore several sparse PCA methods that have been developed over the past few decades and discuss the benefits and drawbacks of each method. We will evaluate each method on a breast tissue data set compiled by JP Marques de Sá at INEB-Instituto de Engenharia Biomédica.

\section*{State of the Art}
% PCA
Jolliffe \citeyear{Jolliffe1986} introduced the ubiquitous dimension-reduction technique PCA in 1986. It has since been a staple in statistics curricula and academic research. As previously mentioned, one of the most relevant drawbacks to using PCA is the difficulty of interpreting separate principal components as they are a linear combination of all the original variables. Since its creation, several attempts have been made at making the principal components more intelligible.\\
\\
% Thresholding
One of those approaches frequently applied is setting loadings with absolute values below a certain threshold to zero. This \textit{thresholding} method could lead to misleading results for a series of reasons as described by Cadima and Jolliffe \citeyear{Cadima1995}.
% Orthogonal rotations
Another common practice based on factor  analysis is to perform and orthogonal rotation on the regular PCA components \cite{Richman1986, Richman1987}. This rotation is then optimized by maximizing simplicity criteria such as \textit{varimax} and \textit{quartimax}. Both these criteria will move towards their maximum when the loadings of the vectors are either close to $0$ or $1$. The drawback of this method is that it does not preserve the successive maximization property inherent to PCA. Moreover Jolliffe \citeyear{Jolliffe1989, Jolliffe1995} warns that orthogonal rotations often lead to non-orthogonal loadings and pairwise correlation between the principal components, and should be reserved for principal components with near-equal variance.\\
\\
% SCoT
This rotation method is two-staged in nature as it requires you to first compute PCA and then rotate the resulting PCs optimally. However, Jolliffe and Uddin \citeyear{Jolliffe2000} introduced the one-stage \textit{SCoT} approach that combines the objectives of variance maximization (like PCA) and simplification (using the varimax criteria). Contrary to a the normal orthogonal rotation approach that stays within the subspace with maximum variance defined by the regular principal directions, SCoT's use of a penalty function will make the resulting principal components divert from this maximum variance subspace and hence faces a variance-simplicity trade-off.\\
\\
% Vines
While SCoT yields increased sparsity compared to PCA, the loadings will still be nonzero values. This is an issue that both Hausman \citeyear{Hausman1982} and Vines \citeyear{Vines2000} aim to solve. Hausman introduced an integer programming solution for assembling principal components using integers loadings from a finite set (typically {$-1$,$0$,$1$}). Each element is determined sequentially as to optimize the variance explained by each component.\\ 
\\
% SCoTLASS
Jolliffe, Trendafilov, and Uddin \citeyear{Jolliffe2003} introduced \textit{SCoTLASS}, which is a modified PCA technique that is based on the least absolute shrinkage and selection operator (or LASSO). Including a LASSO penalty on the loadings helps to induce sparsity, however SCoTLASS is often computationally costly due to its non-convex nature. In addition, hyperparameter tuning for SCoTLASS can be challenging, altogether making SCoTLASS an impractical method for most applications.\\

% SPCA
One of the later developments is sparse PCA (henceforth SPCA) developed by Zou, Hastie and Tibshirani \citeyear{Zou2006}. Like SCoTLASS, this method incorporates the lasso method, but in addition, they reformulate the PCA in a regression-type optimization problem. This method achieves overall sparse loadings with limited variable overlap between the different principal components. However, unlike SCoT and SCoTLASS, SPCA results in non-orthogonal PCs. In the years following its first publication, SPCA has been improved upon in a plethora of ways \cite{Shen2008, Journee2010}.\\
\\
% After SPCA
Developments in the field of sparse PCA after the introduction of SPCA mainly build on the spiked covariance model introduced by Johnstone \citeyear{Johnstone2001}. The model assumes that there exist a small number of natural directions that explains most of the variance. It states that the data is drawn from a multi-Gaussian distribution with zero-mean and a covariance matrix given by $I+\theta vv^T$, where $v$ is a unit norm sparse vector. A significant chunk of the literature is devoted to finding this $v$ in high-dimensionality settings where the number of features $p$ is way larger than the number of observations $n$. In 2013, \citeauthor{Ma2013} presents an estimator for Sparse PCA that attains the convergence, but is confronted with computational bottleneck as the algorithm is not computable in polynomial time. Further research focuses on solving this computational problem\footnote{Such as \citeA{Wang2016, Cai2013, Vu2013, Berthet2013}.}.

\section*{Methodology}

%How did you tackle the question? How did you modify existing methods? What datasets did you use? What theoretical tools did you apply? If you deviated from your original plan, explain why.

\subsection*{Simple Component Technique (SCoT)}
The Simple Component Technique introduced by Jolliffe and Uddin \cite{Jolliffe2000} optimizes a combination of both large variance and simplicity in one stage. Every component is also constrained to be orthogonal and uncorrelated with every other component. The SCoT maximization problem can be written as:
\begin{equation*}
\begin{aligned}
\arg\max_{v_{i}} \quad & \underbrace{v_{i}^{T}\Sigma v_{i}}_{\text{Variance}} + \lambda \underbrace{S(v_{i})}_{\text{Sparsity}}\\
\textrm{s.t.} \quad & \lVert v_{i} \rVert_{2} = 1 \text{ and } v_{i}^{T}v_{j} = 0 \text{ for } j < i < p \\
\textrm{where} \quad & S(v_{i}) = \frac{1}{p}\left[\sum_{k=1}^{p} v_{k}^4- (\sum_{k=1}^{p} v_{k}^2)^2 \right]\\ 
\end{aligned}
\end{equation*}
Where $\Sigma\in\mathbb{R}^{n \times p}$ is the covariance matrix of the zero-mean dataset $X \in \mathbb{R}^{n\times p}$ with $n$ samples and $p$ features. $S(v_i)$ is often referred to as the varimax rotation criteria and simplifies the structure of vector $v_i$ through driving the several loading to zero by maximizing the variance of the squared vector loadings. The important of this sparsity components in the maximization problem is regulated by the term $\lambda$. Since both the variance and sparsity terms in the maximization problem are easily differentiable, we chose to implement this method through the use of gradient descent.

\subsection*{Simple Component Technique-LASSO (SCoTLASS)}
The SCoTLASS method solves the following constrained maximization problem for some regularization parameter $c$:

\begin{equation}
\begin{aligned}
\arg\max_{v_{i}} \quad & v_{i}^{T}\Sigma v_{i}\\
\textrm{s.t.} \quad & \lVert v_{i} \rVert_{2} = 1 \text{, }
            \lVert v_{i} \rVert_{1} < c \text{ and } v_{i}^{T} v_{j} = 0 \text{ for }  j < i < p \\
\end{aligned}
\end{equation}

Similarly to SCoT, we used projected gradient ascent to optimize SCoTLASS. However, the SCoTLASS problem involves an L1 penalty constraint, which is not differentiable at every point and thus poses an issue when using projected gradient ascent.\\
\\
In order to remedy this, we approximated $\lVert v\rVert_{1}$ as $v^T\tanh{(\gamma v)}$ for a large $\gamma$ (specifically, $\gamma = 1000$) and reformatted the objective function to include a smooth exterior penalty function $P(x) = 0.5x(1+\tanh{(\gamma x)})$. This procedure is described in detail by Trendafilov \citeyear{Trendafilov2004}. These modifications result in the following adapted SCoTLASS optimization problem for some regularization parameter $t$ and large $\mu$ (we used $\mu = 1000$):

\begin{equation}
\begin{aligned}
\arg\max_{v_{i}} \quad & v_{i}^{T}\Sigma v_{i} - \mu P(v_{i}^T \tanh{(\gamma v_{i})} - t)\\
\textrm{s.t.} \quad & \lVert v_{i} \rVert_{2} = 1 \text { and } v_{i}^{T}v_{j} = 0 \text{ for }  j < i < p \\
\end{aligned}
\end{equation}

\subsection*{Sparse Principal Components (SPCA)}
SPCA was introduced by Zou et al. \cite{Zou2006} and applies the elastic net to find principal components with sparse loadings. They start form the regular PCA principal components of the zero-mean dataset $X$ with as SVD decomposition $UDV^T$ where $Z=UD$ are the uncorrelated and orthogonal principal components. SPCA will attempt to find a $v_i$ that is a sparse estimate of the $i$th principal component $Z_i = U_iD_{ii}$ regularized with an elastic net. This estimate $v_i$ can be found solving the following maximization problem:
\begin{equation*}
\arg\max_{v_{i}} \quad \|Z_i - X v_i\|^2 + \lambda\|v_i\|^2 + \lambda_1\|v_i\|_1
\end{equation*}
If we look at this equation we recognize a connection between PCA and the regression method. The addition of the positive ridge penalty $\lambda\|v_i\|^2$ ensures that even in the case of a $X$ where is not full-rank because $p>>n$ we will find a unique solution for $v_i$. Zou et al. continue with this similarity between PCA and a elastic net regression and derive the following optimization problem for the $k$ first principal components:
\begin{equation*}
\begin{aligned}
\arg\max_{B} \quad & \sum_{i=1}^{k} \left(\|X\alpha_j-X\beta_j\|^2 + \lambda\|\beta_j\|^2 +\lambda_{1,j} \|\beta_j\|_1\right)\\
\textrm{s.t.} \quad & \|\alpha\|^2 = 1\\
\end{aligned}
\end{equation*}
with $A_{p\times k} = [\alpha_1,\ldots,\alpha_k]$ where $\alpha_i$ initially equals the $i$th principal component $Z_i$ and $B_{p\times k} = [\beta_1,\ldots,\beta_k]$ where $\beta_j\propto Z_i$. In order to numerically solve this maximization problem we first look for $B$ given $A$ and then $A$ given $B$ in an alternating fashion until $A$ converges. The general SPCA algorithm looks like:\vspace*{1mm}\\
\begin{algorithm}[H]
\SetAlgoLined
\KwResult{$V = [v_1,\ldots,v_k]$ where $\|v_j\|=1$}
$A_0 = Z$\;
\While{$\|A_{t+1} - A_{t}\|_F \ne 0$}{
    \For{$\beta_j$ in $B_{t+1}$}{
    $Y_j = X\alpha_j$\;
    $\beta_j = \arg\min_\beta \|Y_j - X\beta_j\|^2 + \lambda\|\beta\|^2 + \lambda_{1,j}\|\beta\|_1$\;
    }
    $X^TXB_{t+1}=UDV^T$\;
    $A_{t+1} = UV^T$\;
}
\For{$\alpha_j$ in $A$}{
$v_j = \alpha_j/\|\alpha_j\|$\;
}
\caption{General SPCA Algorithm}
\label{algo1:SPCA}
\end{algorithm}\vspace*{1mm}
In order to solve line $5$ in the algorithm we apply the LARS-EN method for elastic net regressions introduced by Zou and Hastie \cite{Zou2005} which is itself an adaptation of Efron's \citeyear{Efron2004} LARS method. Essentially, we are able to transform an elastic net problem to a simple lasso problem and apply LARS by constructing an artificial data matrices $X^*$ and $Y^*$ based on $X$, $Y$ and $\lambda$.

\subsection*{Data Set}

Although we originally planned on evaluating each sparse PCA method on a gene expression data set with over 20,000 variables, we decided to use a smaller data set to make it easier to interpret results. In addition, certain methods (such as SCoTLASS) are computationally intractable for high-dimension data sets such as gene expression data sets. Ultimately, we tested each method using a data set of measurements taken on recently excised breast tissue. The data set had 9 variables and 106 examples (see Table \ref{tab1:dta_vars} for descriptions of each variable). \\

\subsection*{Evaluation Criteria}

We evaluated these methods based on three criteria: sparsity, percentage of explained variance (PEV), and efficiency and scalability.\\

\textit{Sparsity:} Since the sparse PCA methods we evaluated often result in nonzero loadings that are very close to zero, we cannot simply count the number of zero loadings as a measure of sparsity. For this reason, we used the \textit{gini coefficient} to measure sparsity of loadings as recommended by \citeA{Hurley2008}. The gini coefficient is a simple measure of weight distribution across a vector. The coefficient takes a value between 0 and 1 inclusive, with a 0 value indicating total equality of loadings and a value of 1 indicating total inequality (i.e. there is only one nonzero loading). Thus, a larger gini coefficient corresponds with higher sparsity. \\

\textit{Percentage of explained variance (PEV):} An effective sparse PCA method will achieve sparsity while maintaining as much explained variance in the first few principal components as possible. For each sparse PCA method, we calculated cumulative PEV (CPEV) of the PCs and plotted the results for comparison purposes. \\

\textit{Efficiency and scalability:} Certain sparse PCA methods work well with small data sets with few variables, but become computationally impractical with higher-dimensional data sets. For each method, we measured run time on the breast tissue data set and analyzed level of complexity to assess the suitability of each method for different data set types.\\

%Results: What results did you obtain? Do they make sense? Provide a thorough analysis. Negative results are completely fine (they can be very valuable!).

\section*{Results}

The table below gives a summary of the results of the different sparse PCA methods for varying regularization parameters.\\

{\color{red}[TABLE HERE]
We want four rows: PCA, SCoT, SCoTLASS, SPCA for their "best" specification. We will show the difference within each model in tables and figures in the appendix. Columns containing: Weighted Gini coefficient, distribution of explained variance, }
\\

We find that, as we begin to constrain loadings, both SCoT and SCoTLASS shift the majority of weight from variable 4 to variable 7. Variable 6 also surprisingly jumps from 6th largest weight in PCA to 3rd largest weight. This indicates that there may be more predictive significance in these two variables than regular PCs would suggest. \\

In terms of efficiency and runtime, the SPCA function takes considerably less time to run than SCoT and SCoTLASS. This is largely due to our use of projected gradient ascent in maximizing SCoT and SCoTLASS, which required very small step sizes over many iterations in order to for the algorithms to converge. SCoTLASS is particularly costly to optimize due to the objective function's nonconvexity. This results in possible local maxima, therefore requiring several random starting points to obtain the global maxima. \\

In optimizing SCoT and SCoTLASS, we found is quite challenging to select the best regularization parameter to yield optimal levels of sparsity and explained variance. There is little guidance from relevant literature on selecting regularization parameters for these methods.\\

In addition, as we varied the regularization parameter for SCoT, we also find that there is a sudden increase in loading sparsity, which is accompanied by a sharp decrease in explained variance. At $\lambda$ = 600, SCoT produces a PC 1 with one zero loading and most other loadings between 0.30 and 0.44 and explains approximately 60\% of variance in the data. At $\lambda$ = 650 however, the PC 1 that results has six zero loadings, with one of the three nonzero loadings carrying nearly all the weight with a 0.98 coefficient, and explains only 20\% of variance in the data. This phenomenon makes SCoT an even more challenging model to tune. 

\section*{Discussion}
%Discussion: What did you find out? Do the results suggest any other interesting questions to explore?

% Pros of SCoT:
% - PCs are orthogonal
% - Computationally simpler than SCoTLASS
% Cons of SCoT:
% - difficult to tune 
%
% Pros of SCoTLASS:
% - PCs are orthogonal
% Cons of SCoTLASS:
% - Costly, requires random starts
% - difficult to tune
% - resulting PCs not as sparse as those resulting from other sparse PCA methods
% - selects at most n zero loadings (problematic when p >> n)
% Pros of SPCA:
%
% Cons of SPCA:
% - produces non orthogonal pcs

% Discussion points:
% - explore ways to create uncorrelated sparse PCAs
% - explore other optimization methods for SCoT and SCoTLASS that are more efficient
% - test methods with higher dimensional data sets

% References
\pagebreak
\bibliographystyle{apacite}
\bibliography{references}

% Appendix
\pagebreak
\appendix
\section*{Appendix}
\begin{table}[htpb!]
\caption{Variable descriptions}
\label{tab1:dta_vars}
\begin{center}
 \begin{tabular}{||c l||}
 \hline
Variable & Variable Description \\ [0.5ex] 
 \hline\hline
 $x_{1}$ & I0 - impedivity (ohm) at zero frequency \\ 
 \hline
 $x_{2}$ & 	PA500 - phase angle at 500 KHz \\
 \hline
 $x_{3}$ & HFS - high-frequency slope of phase angle \\
 \hline
 $x_{4}$ & DA - impedance distance between spectral ends \\
 \hline
 $x_{5}$ & AREA - area under spectrum \\
 \hline
 $x_{6}$ & 	A/DA - area normalized by DA \\
 \hline
 $x_{7}$ & MAX IP - maximum of the spectrum \\
 \hline
 $x_{8}$ & 	DR - distance between I0 and real part of the maximum frequency point \\
 \hline
 $x_{9}$ & P - length of the spectral curve \\ [1ex] 
 \hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[htpb!]
    \centering
    \caption{SCoT Results for different parameters}
    \label{rstl:scot}
    \input{../Tables/SCoT}
\end{table}

\begin{table}[htpb!]
    \centering
    \caption{SCoTLASS Results for different parameters}
    \label{rstl:scotlass}
    \input{../Tables/SCoTLASS}
\end{table}

\begin{table}[htpb!]
    \centering
    \caption{SPCA Results for different parameters}
    \label{rstl:spca}
    \input{../Tables/SPCA}
\end{table}

\end{document}