{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import null_space\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = pd.read_csv(\"../Data/BreastTissue.csv\").drop(['Case #'],axis = 1).drop(['Class'],axis=1).to_numpy()\n",
    "y_small = pd.read_csv(\"../Data/BreastTissue.csv\").drop(['Case #'],axis = 1)['Class'].to_numpy()\n",
    "\n",
    "y_small = preprocessing.LabelEncoder().fit_transform(y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_PCA(X, k = \"all\"): \n",
    "    \"\"\"\n",
    "    function takes an n x p feature matrix\n",
    "    returns two arrays:\n",
    "    - array with percentage of explained variance in first k principal directions (k_comp x 1)\n",
    "    - array with principal directions (k_comp x p)\n",
    "    \"\"\"\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    if k == \"all\": k = min(X.shape[0],X.shape[1])\n",
    "    pca = PCA(n_components = k)\n",
    "    pca.fit(X)\n",
    "    PEVs = pca.explained_variance_ratio_\n",
    "    prin_comp = pca.components_\n",
    "    EVs = pca.explained_variance_\n",
    "    \n",
    "    return PEVs, prin_comp, EVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elasticNet_obj(X, v, reg_param, reg_param1):\n",
    "    return\n",
    "\n",
    "def elasticNet_grad(sigma, v, reg_param, reg_param1):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elasticNetGradDesc(A_i, X, reg_param, reg_param1,\n",
    "                       x0 = 'default', alpha = 1e-3, \n",
    "                       max_iter = 1000, crit = 1e-10):\n",
    "    '''\n",
    "    function takes\n",
    "    - X: n x p dataset\n",
    "    - A: p x n matrix\n",
    "    \n",
    "    function returns\n",
    "    - B: p x n matrix\n",
    "    '''\n",
    "    # Configure the parameters\n",
    "    iters, delta = 1, 1\n",
    "    \n",
    "    # Initialize the algorithm\n",
    "    if x0 == 'default': x0 = np.ones(shape = (num_feat,1))\n",
    "    elif x0 == 'random': x0 = np.random.rand(num_feat,1)\n",
    "    else: pass\n",
    "    v = x0/np.linalg.norm(x0)\n",
    "    \n",
    "    # Calculate initial value of objective function\n",
    "    obj_cache = elasticNet_obj(X, v, reg_param, reg_param1)\n",
    "    \n",
    "     # Projected gradient descent\n",
    "    # Stopping criteria:\n",
    "    # (1) Maximum iterations reached\n",
    "    # (2) Change in objective function negligible\n",
    "    while iters < max_iter and delta > crit:\n",
    "        \n",
    "        # Update the vector\n",
    "        v_new = v - alpha*elasticNet_grad(sigma, v, reg_param, reg_param1)\n",
    "        \n",
    "        # Project loading vector back onto feasible set \n",
    "        # (vectors of l2 norm of 1 that are orthogonal to all other pcs)\n",
    "        v_proj = V @ v_new\n",
    "        v_proj = v_proj/(np.linalg.norm(v_proj))\n",
    "        \n",
    "        # Use the projected loading veactor to retrieve the value of the\n",
    "        # objective function\n",
    "        updated_obj = elasticNet_obj(X, v_proj, reg_param, reg_param1)\n",
    "\n",
    "        # Calculate the difference in value of the objective function\n",
    "        delta = updated_obj - obj_cache\n",
    "        \n",
    "        # Update vector and number of iterations\n",
    "        v, iters, obj_cache = v_proj, iters + 1, udated_obj\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVDProblem(B, X):\n",
    "    '''\n",
    "    function takes\n",
    "    - X: n x p dataset\n",
    "    - B: p x n matrix\n",
    "    \n",
    "    function returns\n",
    "    - A: p x n matrix\n",
    "    '''\n",
    "    U, D, V = np.linalg.svd(X.T @ X @ B)\n",
    "    return U @ V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPCA(X, reg_param, reg_param1, \n",
    "         x0 = 'default', alpha = 1e-3, \n",
    "         max_iter = 1000, crit = 1e-10):\n",
    "    '''\n",
    "    function takes\n",
    "    - X: n x p dataset\n",
    "    - reg_param: regularization parameter (positive float)\n",
    "    - x0: Initial value of the vector\n",
    "    - alpha: Step size of gradient descent (<1 for convergence)\n",
    "    - max_iters: max number of steps (positive integer)\n",
    "    - crit: critical stopping value for the gradient descent algorithm\n",
    "    \n",
    "    function returns\n",
    "    - array with principal components in its columns (n x p)\n",
    "    '''\n",
    "    # (1) Let A start at V[,1:k] the loadings of the first \n",
    "    # k ordinary principal components\n",
    "    A = reg_PCA(X)[1].T\n",
    "    B = np.zeros_like(A)\n",
    "    \n",
    "    iters, delta = 1, 1\n",
    "    while iters < max_iter and delta > crit:\n",
    "        # (2) Given a fixed A = [alpha_1, ..., alpha_k], solve the elastic\n",
    "        # net problem for j = 1, 2, ..., k\n",
    "        for i in range(len(A)):\n",
    "            B[i] = elasticNetGradDesc(A[i], X, reg_param, reg_param1, \n",
    "                                      x0, alpha, max_iter, crit)\n",
    "        \n",
    "        # (3) For a fixed B = [beta_1, ..., beta_k], compute the SVD of \n",
    "        # X^TXB = UDV^T then update A = UV^T\n",
    "        A = SVDProblem(B, X)\n",
    "        \n",
    "        # (4) Repeat steps (2) and (3) until convergence\n",
    "        delta, A_old = np.linalg.norm(A - A_old), A\n",
    "    \n",
    "    # (5) Normalize the altered principal components\n",
    "    return (A/np.linalg.norm(A, axis = 1)).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
