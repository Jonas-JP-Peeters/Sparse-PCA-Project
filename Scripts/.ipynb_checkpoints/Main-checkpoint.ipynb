{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import null_space\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.read_csv(\"../Data/data.csv\").drop(['Unnamed: 0'], axis = 1).to_numpy()\n",
    "#y = pd.read_csv(\"../Data/labels.csv\").drop(['Unnamed: 0'], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_proto = X[1:101, 1:201]\n",
    "#y_proto = y[1:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = pd.read_csv(\"../Data/BreastTissue.csv\").drop(['Case #'],axis = 1).drop(['Class'],axis=1).to_numpy()\n",
    "y_small = pd.read_csv(\"../Data/BreastTissue.csv\").drop(['Case #'],axis = 1)['Class'].to_numpy()\n",
    "\n",
    "y_small = preprocessing.LabelEncoder().fit_transform(y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAObject:\n",
    "    def __init__(self, PCs, X, label):\n",
    "        assert len(PCs) == min(X.shape[0],X.shape[1]), \"Need all the principal components!\"\n",
    "        self.pcs = PCs\n",
    "        self.X = StandardScaler().fit_transform(X)\n",
    "        self.cov = np.cov(self.X.T)\n",
    "        self.label = label\n",
    "        self.pev, self.cev, self.ev = None, None, None\n",
    "        self.nonZeroLoads = None\n",
    "        self.gini = [gini(i) for i in self.pcs]\n",
    "    \n",
    "    def calcExpVar(self):\n",
    "        tot_var = np.sum(np.diag(self.cov))\n",
    "        self.ev = np.diag(self.pcs @ np.cov(self.X.T) @ self.pcs.T)/np.linalg.norm(self.pcs, axis = 1)**2\n",
    "        self.pev = [ev/tot_var for ev in self.ev]\n",
    "        self.cev = np.cumsum(np.flip(np.sort(self.pev)))\n",
    "        \n",
    "    def calcNonZeroLoads(self):\n",
    "        self.nonZeroLoads = [np.count_nonzero(pc) for pc in self.pcs]\n",
    "        \n",
    "    def plotNonZeroLoadtoPEV(self):\n",
    "        self.calcExpVar()\n",
    "        self.calcNonZeroLoads()\n",
    "            \n",
    "        plt.plot(self.nonZeroLoads, self.pev, label=self.label)\n",
    "        plt.xlabel('Number of non-zero loadings')\n",
    "        plt.xscale('log')\n",
    "        plt.ylabel('Percentage of explained variance (PEV)')\n",
    "        plt.legend()\n",
    "        plt.title('Percentage of explained variance (PEV) vs non-zero loadings')\n",
    "        \n",
    "    def plotCEV(self):\n",
    "        self.calcExpVar()\n",
    "        x = np.arange(len(self.pcs))\n",
    "        \n",
    "        plt.plot(x, np.sort(self.cev), label = self.label)\n",
    "        plt.xlabel('Number of component')\n",
    "        plt.ylabel('Cumulative explained variance (CEV)')\n",
    "        plt.legend()\n",
    "        plt.title('Cumulative explained variance (CEV)')\n",
    "        \n",
    "    def plotSparsitytoEV(self,k): #k is which PC you want to plot\n",
    "        x=np.arange(1,.1)\n",
    "        \n",
    "        plt.scatter(self.gini[k], self.ev[k], label = self.label)\n",
    "        plt.xlabel('Sparsity of PC '+str(k+1))\n",
    "        plt.ylabel('Explained Variance of PC '+str(k+1))\n",
    "        plt.xlim((0,1))\n",
    "        plt.legend()\n",
    "        plt.title('Sparsity of PC versus Expained Variance of PC '+str(k+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_PCA(X, k = \"all\"): \n",
    "    \"\"\"\n",
    "    function takes an n x p feature matrix\n",
    "    returns two arrays:\n",
    "    - array with percentage of explained variance in first k principal directions (k_comp x 1)\n",
    "    - array with principal directions (k_comp x p)\n",
    "    \"\"\"\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    if k == \"all\": k = min(X.shape[0],X.shape[1])\n",
    "    pca = PCA(n_components = k)\n",
    "    pca.fit(X)\n",
    "    PEVs = pca.explained_variance_ratio_\n",
    "    prin_comp = pca.components_\n",
    "    EVs = pca.explained_variance_\n",
    "    \n",
    "    return PEVs, prin_comp, EVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_PCA(X, thresh = 1e-1, k = \"all\"):\n",
    "    \"\"\"\n",
    "    function takes\n",
    "    - X: n x p feature matrix\n",
    "    - thresh: float representing this non-zero cutoff\n",
    "    - k: integer for number of principal directions wanted\n",
    "    returns one array:\n",
    "    - array with principal components in its columns (k x p)\n",
    "    \"\"\"\n",
    "    if k == \"all\": k = min(X.shape[0],X.shape[1])\n",
    "\n",
    "    pcs = reg_PCA(X)[1]\n",
    "    pcs = (np.abs(pcs) >= thresh).astype(int) * pcs\n",
    "    \n",
    "    return pcs[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonZeroLoad_PCA(X,j, k = \"max\"):\n",
    "    \"\"\"\n",
    "    function takes\n",
    "    - X: n x p feature matrix\n",
    "    - j: integer for number of non-zero loadings,\n",
    "    - k: integer for number of principal directions wanted\n",
    "    returns three arrays:\n",
    "    - array with percentages of explained variance in first k principal directions (k x 1)\n",
    "    - array with principal directions (k x p)\n",
    "    - array with explained variances\n",
    "    \"\"\"   \n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    if k == \"all\": k = min(X.shape[0],X.shape[1])\n",
    "    \n",
    "    PCA_PEV, PCA_PC, PCA_EV = reg_PCA(X,min(X.shape[0],X.shape[1]))\n",
    "    \n",
    "    total_var = sum(PCA_EV)\n",
    "    \n",
    "    thresh_PCA_PC = np.empty((0,PCA_PC.shape[1]))\n",
    "    thresh_PCA_PEV = []\n",
    "    thresh_PCA_EV = []\n",
    "    \n",
    "    PCA_PC_sorted = np.sort(np.absolute(PCA_PC), axis = 1)\n",
    "    for m in range(k):\n",
    "        thresh = PCA_PC_sorted[m][-j]\n",
    "        thresh_PC = (np.absolute(PCA_PC[m]) >= thresh).astype(int)*PCA_PC[m]\n",
    "        thresh_PCA_PC = np.vstack((thresh_PCA_PC, thresh_PC))\n",
    "    \n",
    "    return thresh_PCA_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpTrans(r, s, k, X_scaled):\n",
    "    \"\"\"\n",
    "    function takes\n",
    "    - Two directions r and s to transform\n",
    "    - An integer k that restricts the possible directions\n",
    "    - The scaled data matrix X\n",
    "    returns two vectors and three floats\n",
    "    - Two new directions r and s\n",
    "    - Two floats representing the variances explained by the new directions r and s\n",
    "    - One float representing the covariance of the new directions r and s\n",
    "    \"\"\"\n",
    "    # Calculate the covariance matrix for the two directions\n",
    "    cov = np.vstack((r,s)) @ np.cov(X_scaled.T) @ np.vstack((r,s)).T\n",
    "    v_old_r, v_old_s, v_old_rs = cov[0,0], cov[1,1], cov[0,1]\n",
    "    \n",
    "    # Get a list of all possible betas\n",
    "    poss_beta1 = [i/2**k for i in range(-2**k, 2**k+1, 1)]\n",
    "    poss_beta2 = [2**k/i for i in range(-2**k, 2**k+1, 1) if i != 0]\n",
    "    poss_beta = np.sort(list(set(poss_beta1 + poss_beta2)))\n",
    "    \n",
    "    # Calculate the norms of the two directions\n",
    "    l2_r, l2_s = np.linalg.norm(r)**2, np.linalg.norm(s)**2\n",
    "    \n",
    "    # Find the beta that maximizes the variance in the normalized\n",
    "    # direction of the first new principal component\n",
    "    v_r, v_s, v_rs = cov[0,0], cov[1,1], cov[0,1]\n",
    "    a = l2_s*v_rs\n",
    "    b = np.sqrt(l2_r*l2_s)*(v_r - v_s)\n",
    "    c = l2_r*v_rs\n",
    "    discr = b**2 - 4*a*c\n",
    "    beta_star = (-b+np.sqrt(discr))/(2*a)\n",
    "    \n",
    "    # Select beta from possible values that's closest to the optimal value\n",
    "    beta_star = min(list(poss_beta), \n",
    "                    key = lambda x:abs(x-beta_star))\n",
    "    \n",
    "    # Calculate the two new directions\n",
    "    if abs(beta_star) <= 1:\n",
    "        r_new = 2**k*r + 2**k*beta_star*s\n",
    "        s_new = 2**k*beta_star*l2_s*r - 2**k*l2_r*s\n",
    "    else:\n",
    "        r_new = 2**k*r/beta_star + 2**k*s\n",
    "        s_new = 2**k*l2_s*r - 2**k*l2_r*s/beta_star\n",
    "    \n",
    "    # Calculate variances and covariance of the new directions\n",
    "    P = np.array([[1, l2_s*beta_star],[beta_star, -l2_r]])\n",
    "    cov_new = P.T @ cov @ P\n",
    "    v_r, v_s, v_rs = cov_new[0,0], cov_new[1,1], cov_new[0,1]\n",
    "    \n",
    "    return r_new, s_new, v_r, v_s, v_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcPairFinder(cov, cache):\n",
    "    \"\"\"\n",
    "    function takes\n",
    "    - A covariance matrix\n",
    "    - A cache containing the indices of excluded pcs\n",
    "    returns two integers:\n",
    "    - i_r: the index of the first principal component\n",
    "    - i_s: the index of the second principal component\n",
    "    \"\"\"\n",
    "    # Turn covariance matrix into a sparse lower triangular matrix\n",
    "    grid = np.tril(cov, k = -1)\n",
    "    vrs = np.diag(cov)\n",
    "    # Flatten the grid and sort from largest cov to smallest cov\n",
    "    srt = np.sort(np.ravel(grid), kind = 'heapsort')\n",
    "    lst = list(np.flip(np.trim_zeros(srt)))\n",
    "    mask = False\n",
    "    \n",
    "    # Find the pair of pcs that are not already in the cache and hav\n",
    "    # the highest covariance in the grid\n",
    "    while not np.any(mask):\n",
    "        covar = lst.pop(0)\n",
    "        index = np.argwhere(grid == covar)\n",
    "        mask = np.ravel(np.invert(np.any(np.isin(index, cache), axis = 1, keepdims = True)))\n",
    "\n",
    "    # Unpack the indices make sure that the variance of r is larger than the variance of s\n",
    "    index = index[mask,:][0]\n",
    "    i_r = index[np.argmax([vrs[i] for i in index])]\n",
    "    i_s = index[np.argmin([vrs[i] for i in index])]\n",
    "    return i_r, i_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_PCA(X, k, iters = 1):\n",
    "    \"\"\"\n",
    "    function takes\n",
    "    - An unscaled data matrix X\n",
    "    - An integer k that restricts the possible directions\n",
    "    - An integer indicating the number of iterations\n",
    "    returns one array:\n",
    "    - array with principal components in its columns (k x p)\n",
    "    \"\"\"\n",
    "    # Retrieve the principal components and their covariance matrix\n",
    "    n_obs, n_feats = X.shape\n",
    "    q = np.identity(n_obs)\n",
    "    for _ in range(n_feats//n_obs):\n",
    "        q = np.hstack((q,np.identity(n_obs)))\n",
    "    pcs = q[:n_obs,:n_feats]\n",
    "    \n",
    "    cov = np.cov(X.T)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Setup up globals for loop\n",
    "    cov_pc = pcs @ cov @ pcs.T\n",
    "    cache = []\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        while len(cache) <= len(pcs) - 2:\n",
    "            clear_output(wait = True)\n",
    "            print(\"Working on principal component \", len(cache) + 2,\"/\",len(pcs))\n",
    "            \n",
    "            # Find set of principal components to transform\n",
    "            i_r, i_s = pcPairFinder(cov_pc, cache)\n",
    "            r, s = pcs[i_r], pcs[i_s]\n",
    "            cache += [i_r, i_s]\n",
    "            \n",
    "            # Transform the pair of components\n",
    "            print(\"Determining new directions...\")\n",
    "            r_new, s_new, v_r, v_s, v_rs = SimpTrans(r, s, k, X_scaled)\n",
    "\n",
    "            # Update the principal components\n",
    "            pcs[i_r] = r_new\n",
    "            pcs[i_s] = s_new\n",
    "            \n",
    "            # Update the grid matrix\n",
    "            cov_pc[i_s, i_r], cov_pc[i_r, i_s] = v_rs, v_rs\n",
    "    \n",
    "    return pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scotlass_pen(x, gamma):\n",
    "    '''\n",
    "    regularization penalty function for SCoTLASS\n",
    "    \n",
    "    function takes \n",
    "    - x: float\n",
    "    - gamma: float\n",
    "    \n",
    "    function returns \n",
    "    - penalty value: float\n",
    "    '''\n",
    "    return (0.5 * x) * (1 + np.tanh(gamma*x))\n",
    "\n",
    "def scotlass_obj(sigma, v, reg_param, gamma):\n",
    "    '''\n",
    "    objective function for SCoTLASS\n",
    "    \n",
    "    function takes\n",
    "    - sigma: p x p covariance matrix\n",
    "    - v: p x 1 vector\n",
    "    - reg_param: regularization parameter (positive float)\n",
    "    - gamma: float\n",
    "    \n",
    "    function returns\n",
    "    - objective function value at v (p x 1 vector)\n",
    "    '''\n",
    "    varimax = (0.5*v.T) @ sigma @ v\n",
    "    argpen = v.T @ np.tanh(1000*v)-reg_param\n",
    "    penalty = gamma * scotlass_pen(argpen, gamma)\n",
    "    return np.array(varimax - penalty).flatten()\n",
    "\n",
    "def scotlass_grad(sigma, v, reg_param, gamma):\n",
    "    '''\n",
    "    gradient of objective function for SCoTLASS\n",
    "    \n",
    "    function takes\n",
    "    - sigma: p x p covariance matrix\n",
    "    - v: p x 1 vector\n",
    "    - reg_param: regularization parameter (positive float)\n",
    "    - gamma: float\n",
    "    \n",
    "    function returns\n",
    "    - gradient of objective function at v\n",
    "    '''\n",
    "    # Setup of parameters\n",
    "    mu = 1000\n",
    "    \n",
    "    # Calculate floats\n",
    "    y = (v.T @ np.tanh(gamma*v))-reg_param\n",
    "    q = 1 + np.tanh(gamma*y) + gamma*np.cosh(gamma*y)**(-2)*y\n",
    "    \n",
    "    # Calculate vectors\n",
    "    z = np.tanh(gamma*v) + gamma * (np.diag(np.ravel(np.cosh(gamma*v)**(-2))) @ v)\n",
    "    \n",
    "    return (sigma @ v) - 0.5*mu*q*z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scotlassGradAsc(sigma, V, reg_param , x0 = 'default', \n",
    "                    alpha = 10**-3, max_iter = 20000, crit = 1e-1000):\n",
    "    '''\n",
    "    function takes\n",
    "    - sigma: p x p covariance matrix\n",
    "    - reg_param: regularization parameter (positive float); Default = sqrt(p)\n",
    "    - x0: Initial value of the vector\n",
    "    - alpha: Step size of gradient descent (<1 for convergence)\n",
    "    - max_iters: max number of steps (positive integer)\n",
    "    - crit: critical stopping value for the gradient descent algorithm\n",
    "    \n",
    "    function returns\n",
    "    - v: first sparse principal direction (array of length p)\n",
    "    - variance of data along v\n",
    "    '''\n",
    "    # Configure the parameters\n",
    "    iters, delta, gamma = 1, 1, 1000\n",
    "    num_obs, num_feat = sigma.shape\n",
    "    \n",
    "    # Configure the regularization parameter\n",
    "#     reg_error = '''\n",
    "#     The regularization parameter needs to be smaller than \n",
    "#     the square root of the number of features in the dataset.\n",
    "#     '''\n",
    "#     if reg_param == 'default': reg_param = np.sqrt(num_feat)\n",
    "#     else: assert reg_param <= np.sqrt(num_feat), reg_error\n",
    "    \n",
    "    # Initialize the algorithm\n",
    "    if x0 == 'default': x0 = np.ones(shape = (num_feat,1))\n",
    "    elif x0 == 'random': x0 = np.random.rand(num_feat,1)\n",
    "    else: pass\n",
    "    v = x0/np.linalg.norm(x0)\n",
    "    \n",
    "    # Projected gradient descent\n",
    "    # Stopping criteria:\n",
    "    # (1) Maximum iterations reached\n",
    "    # (2) Change in objective function negligible\n",
    "    while iters < max_iter and delta > crit:\n",
    "        \n",
    "        # Update the vector\n",
    "        v_new = v + alpha*scotlass_grad(sigma, v, reg_param, gamma)\n",
    "        \n",
    "        # Project loading vector back onto feasible set \n",
    "        # (vectors of l2 norm of 1 that are orthogonal to all other pcs)\n",
    "        v_proj = V @ v_new\n",
    "        v_proj = v_proj/(np.linalg.norm(v_proj))\n",
    "        \n",
    "        # Use the projected loading vector to retrieve the value of the\n",
    "        # objective function\n",
    "        old_obj = scotlass_obj(sigma, v, reg_param, gamma)[0]\n",
    "        updated_obj = scotlass_obj(sigma, v_proj, reg_param, gamma)[0]\n",
    "\n",
    "        # Calculate the difference in value of the objective function\n",
    "        delta = [updated_obj - old_obj][0]\n",
    "        #print(delta)\n",
    "        # Update vector and number of iterations\n",
    "        v, iters = v_proj, iters + 1\n",
    "    \n",
    "    # return loadings array v\n",
    "    print('finished pc')\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCoTLASS(X, reg_param, x0 = 'default', alpha = 10**-3,\n",
    "             max_iter = 20000, crit = 1e-1000):\n",
    "    '''\n",
    "    function takes\n",
    "    - X: n x p dataset\n",
    "    - reg_param: regularization parameter (positive float); Default = sqrt(p)\n",
    "    - x0: Initial value of the vector\n",
    "    - alpha: Step size of gradient descent (<1 for convergence)\n",
    "    - max_iters: max number of steps (positive integer)\n",
    "    - crit: critical stopping value for the gradient descent algorithm\n",
    "    \n",
    "    function returns\n",
    "    - array with principal components in its columns (n x p)\n",
    "    '''\n",
    "    # Standardize the dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    sigma = np.cov(X.T)\n",
    "    num_feat = X.shape[1]\n",
    "    num_samp = X.shape[0]\n",
    "    \n",
    "    # Initialize an array for principal components\n",
    "    pcs = np.zeros(shape = (min(X.shape[0],X.shape[1]),X.shape[1]))\n",
    "    \n",
    "    # Initialize projection vector V\n",
    "    V = np.identity(num_feat)\n",
    "    \n",
    "    for _ in range(len(pcs)):\n",
    "        # Find the best principal component\n",
    "        pc = scotlassGradAsc(sigma, V, reg_param, x0, alpha, max_iter, crit)\n",
    "        pcs[_] = pc.T\n",
    "        \n",
    "        # Project the covariance matrix and x0 \n",
    "        # on the orthogonal complement of previous pcs\n",
    "        V = null_space(pcs[:_+1]) @ null_space(pcs[:_+1]).T\n",
    "        sigma = V @ sigma\n",
    "        x0 = V @ np.ones(shape = (num_feat,1))\n",
    "    return pcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $B\\in \\mathbb{R}^{p\\times m}$:\n",
    "$$\n",
    "Varimax(B) = \\frac{1}{p^2}\\sum_{k=1}^m\\left[\\underbrace{m\\sum_{j=1}^p b_{jk}^4}_{P} - \\underbrace{\\left(\\sum_{j=1}^p b_{jk}^2\\right)^2}_{Q}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varimax(x):\n",
    "    '''\n",
    "    varimax penalty function\n",
    "    \n",
    "    function takes 1 x p vector\n",
    "    \n",
    "    function returns float\n",
    "    '''\n",
    "    #     p, m = B[None,:].shape\n",
    "    #     P = m*np.sum(np.power(B, 4), axis = 0)\n",
    "    #     Q = np.power(np.sum(np.power(B, 2), axis = 0), 2)\n",
    "    #     return 1/p**2 * np.sum(P - Q)\n",
    "    \n",
    "    p = len(x)\n",
    "    varimax = np.sum(np.power(x, 4))\n",
    "    varimax -= (x.T @ x)[0]**2\n",
    "    varimax = varimax/(p**2)\n",
    "    \n",
    "    return varimax\n",
    "\n",
    "def varimax_grad(x):\n",
    "    '''\n",
    "    varimax penalty function gradient\n",
    "    \n",
    "    function takes 1 x p vector\n",
    "    \n",
    "    function returns 1 x p varimax gradient vector'''\n",
    "    p = len(x)\n",
    "    grad = 4*(np.power(x, 3) - (x.T @ x)* x)/(p**2)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def scot_obj(sigma, v, reg_param):\n",
    "    '''\n",
    "    SCoT objective function\n",
    "    \n",
    "    function takes\n",
    "    - sigma: p x p covariance matrix\n",
    "    - v: 1 x p vector\n",
    "    - reg_param: varimax regularization parameter (positive float)\n",
    "    \n",
    "    function returns float\n",
    "    '''\n",
    "    obj = v.T @ sigma @ v + (reg_param*varimax(v))[0]\n",
    "\n",
    "    return obj\n",
    "\n",
    "def scot_obj_grad(sigma, v, reg_param):\n",
    "    '''\n",
    "    SCoT objective function gradient\n",
    "    \n",
    "    function takes\n",
    "    - sigma: p x p covariance matrix\n",
    "    - v: 1 x p vector\n",
    "    - reg_param: varimax regularization parameter (positive float)\n",
    "    \n",
    "    function returns 1 x p gradient vector\n",
    "    '''\n",
    "    \n",
    "    p = len(v)\n",
    "    grad = np.array((sigma @ v)) + reg_param*varimax_grad(v)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scotGradAsc(sigma, V, reg_param, x0 = 'default', \n",
    "                    alpha = 10**-3, max_iter = 20000, crit = 1e-1000):\n",
    "    '''\n",
    "    function takes\n",
    "    - sigma: p x p covariance matrix\n",
    "    - reg_param: regularization parameter (positive float); Default = sqrt(p)\n",
    "    - x0: Initial value of the vector\n",
    "    - alpha: Step size of gradient descent (<1 for convergence)\n",
    "    - max_iters: max number of steps (positive integer)\n",
    "    - crit: critical stopping value for the gradient descent algorithm\n",
    "    \n",
    "    function returns\n",
    "    - v: first sparse principal direction (array of length p)\n",
    "    - variance of data along v\n",
    "    '''\n",
    "    # Configure the parameters\n",
    "    iters, delta, gamma = 1, 1, 1000\n",
    "    num_obs, num_feat = sigma.shape\n",
    "    \n",
    "    # Initialize the algorithm\n",
    "    if x0 == 'default': x0 = np.ones(shape = (num_feat,1))\n",
    "    elif x0 == 'random': x0 = np.random.rand(num_feat,1)\n",
    "    else: pass\n",
    "    v = x0/np.linalg.norm(x0)\n",
    "    \n",
    "    # Projected gradient descent\n",
    "    # Stopping criteria:\n",
    "    # (1) Maximum iterations reached\n",
    "    # (2) Change in objective function negligible\n",
    "    while iters < max_iter and delta > crit:\n",
    "        \n",
    "        # Update the vector\n",
    "        v_new = v + alpha*scot_obj_grad(sigma, v, reg_param)\n",
    "        \n",
    "        # Project loading vector back onto feasible set \n",
    "        # (vectors of l2 norm of 1 that are orthogonal to all other pcs)\n",
    "        v_proj = V @ v_new\n",
    "        v_proj = v_proj/(np.linalg.norm(v_proj))\n",
    "        \n",
    "        # Use the projected loading vector to retrieve the value of the\n",
    "        # objective function\n",
    "        old_obj = scot_obj(sigma, v, reg_param)[0]\n",
    "        updated_obj = scot_obj(sigma, v_proj, reg_param)[0]\n",
    "\n",
    "        # Calculate the difference in value of the objective function\n",
    "        delta = [updated_obj - old_obj][0]\n",
    "        \n",
    "        # Update vector and number of iterations\n",
    "        v, iters = v_proj, iters + 1\n",
    "    \n",
    "    # return loadings array v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCoT(X, reg_param, x0 = 'default', alpha = 10**-3,\n",
    "             max_iter = 20000\n",
    "         \n",
    "         , crit = 1e-1000):\n",
    "    '''\n",
    "    function takes\n",
    "    - X: n x p dataset\n",
    "    - reg_param: regularization parameter (positive float)\n",
    "    - x0: Initial value of the vector\n",
    "    - alpha: Step size of gradient descent (<1 for convergence)\n",
    "    - max_iters: max number of steps (positive integer)\n",
    "    - crit: critical stopping value for the gradient descent algorithm\n",
    "    function returns\n",
    "    \n",
    "    - array with principal components in its columns (n x p)\n",
    "    '''\n",
    "    # Standardize the dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    sigma = np.cov(X.T)\n",
    "    num_feat = sigma.shape[0]\n",
    "    \n",
    "    # Initialize an array for principal components\n",
    "    pcs = np.zeros(shape = (min(X.shape[0],X.shape[1]),X.shape[1]))\n",
    "    \n",
    "    # Initial vector for gradient descent\n",
    "    if x0 == 'default': x0 = np.ones(shape = (num_feat,1))\n",
    "    elif x0 == 'random': x0 = np.random.rand(num_feat,1)\n",
    "    else: pass\n",
    "    v = x0/np.linalg.norm(x0)\n",
    "    \n",
    "    V = np.identity(X.shape[1])\n",
    "    \n",
    "    for _ in range(len(pcs)):\n",
    "        # Find the best principal component\n",
    "        pc = scotGradAsc(sigma, V, reg_param, x0, alpha, max_iter, crit)\n",
    "        pcs[_] = pc.T\n",
    "        \n",
    "        # Project the covariance matrix and x0 \n",
    "        # on the orthogonal complement of previous pcs\n",
    "        V = null_space(pcs[:_+1]) @ null_space(pcs[:_+1]).T\n",
    "        sigma = V @ sigma\n",
    "        x0 = V @ np.ones(shape = (num_feat,1))\n",
    "        x0 = x0/np.linalg.norm(x0)\n",
    "        \n",
    "    return pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVDProblem(B, X):\n",
    "    '''\n",
    "    function takes\n",
    "    - X: n x p dataset\n",
    "    - B: p x n matrix\n",
    "    \n",
    "    function returns\n",
    "    - A: p x n matrix\n",
    "    '''\n",
    "    U, D, V = np.linalg.svd(X.T @ X @ B)\n",
    "    return U @ V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPCA(X, reg_param, reg_param1, max_iter = 1000, delta = 1e-10):\n",
    "    '''\n",
    "    function takes\n",
    "    - X: n x p dataset\n",
    "    - reg_param: regularization parameter (positive float)\n",
    "    - x0: Initial value of the vector\n",
    "    - alpha: Step size of gradient descent (<1 for convergence)\n",
    "    - max_iters: max number of steps (positive integer)\n",
    "    - crit: critical stopping value for the gradient descent algorithm\n",
    "    \n",
    "    function returns\n",
    "    - array with principal components in its columns (n x p)\n",
    "    '''\n",
    "    # (1) Let A start at V[,1:k] the loadings of the first \n",
    "    # k ordinary principal components\n",
    "    A = reg_PCA(X)[1].T\n",
    "    B = np.zeros_like(A)\n",
    "    \n",
    "    # Normalize the dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    iters, delta = 1, 1\n",
    "    while iters < max_iter and delta > crit:\n",
    "        # (2) Given a fixed A = [alpha_1, ..., alpha_k], solve the elastic\n",
    "        # net problem for j = 1, 2, ..., k\n",
    "        for i in range(len(A)):\n",
    "            B[i] = LARS_EN(X @ A[i].reshape(-1,1), # Target variable\n",
    "                           X,                      # Data\n",
    "                           reg_param,              # l2-norm reg param\n",
    "                           reg_param1)             # l1-norm reg param\n",
    "        \n",
    "        # (3) For a fixed B = [beta_1, ..., beta_k], compute the SVD of \n",
    "        # X^TXB = UDV^T then update A = UV^T\n",
    "        A = SVDProblem(B, X)\n",
    "        \n",
    "        # (4) Repeat steps (2) and (3) until convergence\n",
    "        delta, A_old = np.linalg.norm(A - A_old), A\n",
    "    \n",
    "    # (5) Normalize the altered principal components\n",
    "    return (A/np.linalg.norm(A, axis = 1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 (B given A):\n",
    "$$\n",
    "\\hat{\\beta}_j = argmin_{\\beta_j} \\|Y_j - X\\beta_j\\|^2 + \\lambda\\|\\beta_j\\|^2 + \\lambda_{1,j}\\|\\beta_j\\|_1\n",
    "$$\n",
    "with $Y_j = X\\alpha_j$. We can rewrite this as\n",
    "$$\n",
    "\\hat{\\beta}^* = argmin_{\\beta^*} \\|Y^*_j - X^*\\beta^*\\|^2 + \\gamma\\|\\beta^*\\|_1\n",
    "$$\n",
    "with\n",
    "$$\n",
    "X^* = (1 + \\lambda)^{-1/2}\\begin{pmatrix}X\\\\\\sqrt{\\lambda I}\\end{pmatrix}\\in\\mathbb{R}^{(n+p)\\times p},\\qquad Y^* = \\begin{pmatrix}Y\\\\0\\end{pmatrix}\\in\\mathbb{R}^{n+p}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\gamma = \\frac{\\lambda_{1,j}}{\\sqrt{1+\\lambda}}\n",
    "$$\n",
    "From $\\beta^*$ we can find $\\hat{\\beta_j}$:\n",
    "$$\n",
    "\\hat{\\beta}=\\frac{1}{\\sqrt{1+\\lambda}}\\hat{\\beta}^*\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LARS_EN(Y, X, reg_param, reg_param1):\n",
    "    '''\n",
    "    function takes\n",
    "    - Y: p x 1 target variable\n",
    "    - X: n x p dataset\n",
    "    - reg_param: regularization parameter for l2-norm\n",
    "    - reg_param1: regularization parameter for l1-norm\n",
    "    \n",
    "    function returns\n",
    "    - beta: 1 x p vector with coefficients\n",
    "    '''\n",
    "    # Find the number of features\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    # Create the artificial dataset for the naïve elastic net\n",
    "    X = np.power(1 + reg_param, -0.5) * np.vstack((X, np.sqrt(reg_param)*np.identity(p)))\n",
    "    Y = np.vstack((Y, np.zeros(shape = (p,1))))\n",
    "    gamma = reg_param1/np.sqrt(1 + reg_param)\n",
    "    \n",
    "    # Use the LARS (Efron 2004) algorithm to solve this lasso regression\n",
    "    lasso = LassoLars(alpha = gamma,\n",
    "                      fit_intercept = False,\n",
    "                      normalize = False,\n",
    "                      max_iter = 1000)\n",
    "    lasso.fit(X, Y)\n",
    "    \n",
    "    # Transform the found coefficients in the elastic net coefficients\n",
    "    beta = lasso.coef_/(1 + reg_param)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5614783e3a3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreg_SPCA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCAObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_small\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_small\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SPCA\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-11535b4b0586>\u001b[0m in \u001b[0;36mSPCA\u001b[1;34m(X, reg_param, reg_param1)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0miters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[0miters\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mcrit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m# (2) Given a fixed A = [alpha_1, ..., alpha_k], solve the elastic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# net problem for j = 1, 2, ..., k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_iter' is not defined"
     ]
    }
   ],
   "source": [
    "reg_SPCA = PCAObject(SPCA(X_small, 1, 1), X_small, \"SPCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE TO PROPERLY TUNE STEP SIZE TO REG_PARAM!!!!!\n",
    "# MAKE SURE TO PROPERLY TUNE STEP SIZE TO REG_PARAM!!!!!\n",
    "# MAKE SURE TO PROPERLY TUNE STEP SIZE TO REG_PARAM!!!!!\n",
    "\n",
    "reg_thresh = PCAObject(threshold_PCA(X_small), X_small, \"Thresh\")\n",
    "reg_reg = PCAObject(reg_PCA(X_small)[1], X_small, \"Regular\")\n",
    "reg_scot_100 = PCAObject(SCoT(X_small, 100, alpha = 10**-3), X_small, \"SCoT 100\")\n",
    "reg_scot_500 = PCAObject(SCoT(X_sxmall, 500, alpha = 10**-3), X_small, \"SCoT 500\")\n",
    "reg_scotlass_1 = PCAObject(SCoTLASS(X_small,1,alpha = 10**-6, ), X_small, \"SCoTLASS 1\")\n",
    "reg_scotlass_2 = PCAObject(SCoTLASS(X_small,2,alpha = 10**-5, max_iter=50000), X_small, \"SCoTLASS 2\")\n",
    "reg_scotlass_2pt5= PCAObject(SCoTLASS(X_small,2.5,alpha = 10**-5, max_iter=50000), X_small, \"SCoTLASS 2.5\")\n",
    "reg_scotlass_4 = PCAObject(SCoTLASS(X_small, 4,alpha = 10**-3), X_small, \"SCoTLASS 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scotlass_1pt5= PCAObject(SCoTLASS(X_small,1.5,alpha = 10**-6, max_iter=100000), X_small, \"SCoTLASS 1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scotlass_1pt6= PCAObject(SCoTLASS(X_small,1.6,alpha = 2*10**-6, max_iter=100000), X_small, \"SCoTLASS 1.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scotlass_1pt7= PCAObject(SCoTLASS(X_small,1.7,alpha = 4*10**-6, max_iter=100000), X_small, \"SCoTLASS 1.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scotlass_1pt8= PCAObject(SCoTLASS(X_small,1.8,alpha = 5*10**-6, max_iter=100000), X_small, \"SCoTLASS 1.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_thresh.plotCEV()\n",
    "reg_reg.plotCEV()\n",
    "reg_scot_100.plotCEV()\n",
    "reg_scot_500.plotCEV()\n",
    "reg_scotlass_2.plotCEV()\n",
    "reg_scotlass_4.plotCEV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_reg.calcExpVar()\n",
    "reg_scot_100.calcExpVar()\n",
    "reg_scot_500.calcExpVar()\n",
    "reg_scotlass_1.calcExpVar()\n",
    "reg_scotlass_1pt5.calcExpVar()\n",
    "reg_scotlass_1pt6.calcExpVar()\n",
    "reg_scotlass_1pt7.calcExpVar()\n",
    "reg_scotlass_1pt8.calcExpVar()\n",
    "reg_scotlass_2.calcExpVar()\n",
    "reg_scotlass_2pt5.calcExpVar()\n",
    "\n",
    "reg_reg.plotSparsitytoEV(0)\n",
    "reg_scotlass_1.plotSparsitytoEV(0)\n",
    "reg_scotlass_1pt5.plotSparsitytoEV(0)\n",
    "reg_scotlass_1pt6.plotSparsitytoEV(0)\n",
    "reg_scotlass_1pt7.plotSparsitytoEV(0)\n",
    "reg_scotlass_1pt8.plotSparsitytoEV(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(reg_reg.pcs))\n",
    "\n",
    "vm_reg = np.apply_along_axis(varimax, axis = 1, arr = reg_reg.pcs)\n",
    "vm_scot = np.apply_along_axis(varimax, axis = 1, arr = reg_scot.pcs)\n",
    "\n",
    "plt.plot(x, vm_reg, label = \"regular\")\n",
    "plt.plot(x, vm_scot, label = \"SCoT\")\n",
    "plt.xlabel('Number of component')\n",
    "plt.ylabel('Varimax criterion')\n",
    "plt.legend()\n",
    "plt.title(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(reg_scotlass_1pt8.pcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.apply_along_axis(varimax, axis = 1, arr = reg_scotlass_14.pcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.apply_along_axis(varimax, axis = 1, arr = reg_scot.pcs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gini(x):\n",
    "#     # (Warning: This is a concise implementation, but it is O(n**2)\n",
    "#     # in time and memory, where n = len(x).  *Don't* pass in huge\n",
    "#     # samples!)\n",
    "\n",
    "#     # Mean absolute difference\n",
    "#     mad = np.abs(np.subtract.outer(x, x)).mean()\n",
    "#     # Relative mean absolute difference\n",
    "#     rmad = mad/np.mean(x)\n",
    "#     # Gini coefficient\n",
    "#     g = 0.5 * rmad\n",
    "#     return g\n",
    "\n",
    "# gini coefficient method from Olivia Guest: https://github.com/oliviaguest/gini\n",
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # based on bottom eq:\n",
    "    # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n",
    "    # from:\n",
    "    # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    # All values are treated equally, arrays must be 1d:\n",
    "    array = array.flatten()\n",
    "    if np.amin(array) < 0:\n",
    "        # Values cannot be negative:\n",
    "        array -= np.amin(array)\n",
    "    # Values cannot be 0:\n",
    "    array += 0.0000001\n",
    "    # Values must be sorted:\n",
    "    array = np.sort(array)\n",
    "    # Index per array element:\n",
    "    index = np.arange(1,array.shape[0]+1)\n",
    "    # Number of array elements:\n",
    "    n = array.shape[0]\n",
    "    # Gini coefficient:\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
